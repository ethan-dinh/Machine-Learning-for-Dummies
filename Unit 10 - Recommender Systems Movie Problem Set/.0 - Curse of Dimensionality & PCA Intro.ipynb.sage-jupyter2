{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":80347136},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"type":"settings"}
{"cell_type":"markdown","id":"27ef7f","input":"The covariance matrix, C, can be computed for a data set with zero mean by calculating $Cov=\\frac{1}{n-1}X X^T$.\n\nPrincipal component analysis (PCA) is usually explained via an eigen-decomposition of the covariance matrix. \n\nSingular Value Decomposition (SVD) is a related decomposition that can be used to solve the PCA problem as well, and with better numeric properties.\n\nWe can obtain the decomposition of the covariance matrix by performing singular value decomposition (SVD) on the data matrix ùêó:\n\n$$Cov(X) = \\frac{1}{n-1} X^TX$$\n\n$$X = U\\Sigma V^T$$\n\n$$Cov(X) = Cov(U\\Sigma V^T)$$\n\n$$\\frac{1}{n-1}X^TX = \\frac{1}{n-1} (U\\Sigma V^T)(U\\Sigma V^T)^T$$  \n\n$$\\frac{1}{n-1}X^TX = \\frac{1}{n-1} U\\Sigma V^T V \\Sigma U^T$$  \n\nbut $V^T V = I$ since the eigenvectors are orthonormal\n\nSo $$X^TX = U \\Sigma^2 U^T$$\n\nIn summary, by decomposing $X$ into its SVD decomposition, we have obtained the eigenvalue decomposition of the covariance matrix, $X^TX$, which gives us the Principal Components.","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"5c925b","input":"Sources:\n    \nhttps://medium.com/diogo-menezes-borges/give-me-the-antidote-for-the-curse-of-dimensionality-b14bce4bf4d2\n\nImagine you created a classifier that helps you distinguish üê± from üê∂ according to some features you gave, such as snout‚Äôs length, paw size, weight, color and type of fur. You have five features that in combination could be used by a classification algorithm to help you classify your samples. You start to think that maybe you can improve the classifier‚Äôs results if you just add more features based on other distinguishable characteristics. Maybe you can but probably you won‚Äôt. **As the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially.**\n\n<img src=\"images/dim1.png\" width=500>\n\n### Understanding Dimensionality\n\nBefore going into details about the curse let‚Äôs understand the impact of dimensionality has on our datasets. Let‚Äôs imagine you have five boxes (dataset training observations). We want each one of these boxes to represent a part of that one-dimension, so we uniformly distribute them along a single line (feature X). At the moment, each one of these boxes owns an equal size subset (1/5) of this segment.\n\n<img src=\"images/dim2.png\" width=500>\n\nNow, let‚Äôs add another dimension (feature Y) and move into a two-dimensional space. It is desirable to maintain the same distance relationship between our data points, as in the one-dimension space. Since now we‚Äôre in a two-dimension space we must fill all empty spaces with more boxes so that we can maintain this distance between samples. In the end, we will end up with 25 data points available with each point occupying 1/25 of the space.\n\nGoing from one-dimension to two-dimensions altered the total number of data points from 5 into 25. What happens if we go into three-dimensional space? In order to cover the whole dimensional space, maintaining the same distance between points, we will end up having to find 125 data points ( 5¬≥ = boxes) with each point occupying 1/125 of the space. Therefore, the growth is exponential and everytime you increase the number of dimensions, more boxes will have to be added to fill the empty spaces! Add another dimension and you‚Äôll get 645 boxes, another one and there‚Äôs 3125, six dimensions and you see 15625, and so forth.\n\n### The curse of dimensionality and overfitting\n\nIn the previous example we saw that at every higher dimension the number of data points (boxes) in our space had to increase as well in order to maintain the same distance relationship between them. Usually, in real-life problem this is an issue. When you add a new feature to your model sometimes not enough data will be added to maintain previous relations and thus the new feature might not have a positive impact on the classifier. Therefore, you end up making it more complex without taking any advantage of it.\n\nLet‚Äôs go back to our cats and dogs example to understand this so-called curse. You start by building a classifier that only takes into account a single feature, for example, the snout‚Äôs length. The separation is not perfect but our classifier does classify each animal as a dog or a cat.\n\n<img src=\"images/dim3.png\" width=500>\n\nNevertheless, we believe this classifier can do better! So, you decide to add a new feature (e.g. paw size). It becomes clear to us that only one feature wouldn‚Äôt cut it since we obtain better results with the introduction of the latter. However, with this feature, we still cannot make a perfect linear separation between our observations.\n\n<img src=\"images/dim4.png\" width=500>\n\nFollowing this line of thought, there‚Äôs a high probability our model‚Äôs classification capacity will increase if we add another feature. Let‚Äôs do that by adding the weight! Great! By adding the last feature our classifier is able to create a linear combination of the three features in order to obtain perfect classification results on our training dataset.\n\n<img src=\"images/dim5.png\" width=500>\n\nFrom this example, one would conclude that more features you had the better our classifier will perform, correct? WRONG! As shown by the first image on this post, there‚Äôs an optimal number of features to achieve. Above that, the classifier will lose performance. Moreover, note that the density of our training samples decreased exponentially as we increased the dimensionality of the problem which also can become a problem.\n\n\nAs we add more features, the space between the observations grows, and it becomes sparser and sparser. This sparsity helps our classifier to classify our observations since it becomes more easy to find a separable hyperplane due to the fact that the likelihood of our training samples lying on the wrong side of the best hyperplane becomes infinitely small as you increase the number of features to infinite. However, by projecting this high dimensional classification into a lower dimensional space we are struck by an evident problem in Machine Learning: **Overfitting**!\n\n<img src=\"images/dim6.png\" width=500>\n\nAlthough our data was linearly separable in a three-dimensional space, this is not the case when we lower one dimension in the feature space. In fact, when we added the third feature the classifier learned the appearance of a specific instance and exceptions of our training dataset, hence overfitting and dooming our classifier to failure if tested with real-world data. Thus, we will get a better classifier by only using two features, since it will generalise better to any future dataset.\n\n<img src=\"images/dim7.png\" width=500>\n\n","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"d15da1","input":"### Avoiding this curse\n\nBefore going any further, have in mind that there is no fixed rule that defines how many features should be used in a regression/classification problem. The magic number depends on the amount of training data available, the complexity of the decision boundaries and the type of classifier used. For example, if a theoretical number of training examples was available, such as in our example of the boxes we would not have to deal with this curse and we could simply use an infinite number of features to obtain the perfect classification.\n\nOne main approach to reduce dimensionality is projection:\n\n### Projection\n\nAs stated before, ‚Äúin most real-world problems training instances are not spread uniformly across all dimensions. You might have features that are constant while others are highly correlated which in the end makes all training instances lie within (or close to) a much lower-dimensional subspace of the high-dimensional space.‚Äù ‚Äî Hands-on Machine Learning with Scikit-Learn & TensorFlow.\n\nConsider the Swiss roll in Figure A whose points are plotted discretely in Figure B. If we project the points in Figure B perpendicularly onto to the 2D X-Y space then we get the image in Figure C. This approach is not always the best for dimensionality reduction. For example, the famous Swiss roll shown below is a good example why this approach is not good because by dropping the axis Z and projecting the instances in a 2D environment, we end up squashing the different layers of the Swiss roll together (Figure C). The desired simplification would be to unroll the Swiss roll to obtain the 2D image in Figure D.\n\n<img src=\"images/dim8.png\" width=500>\n\nExamples of this approach are Singular Value Decomposition (SVD), Principal Component Analysis (PCA), and Linear Discriminant Analysis (LDA). We've already covered SVD and we'll cover PCA now.\n\n### Principal Component Analysis\nIt starts by identifying the hyperplane that lies closest to the data and then it projects the data onto it.\n- **Maximizes Variance** : Finds direction/dimension of maximum variance of the data.\n- **Orthogonal** : Finds directions which are orthogonal (perpendicular to the first component that it found).\n\nConsider the following image:\n\n<img src=\"images/dim9.png\" width=500>\n\nWe have on the left a representation of a simple 2D dataset with three 1D hyperplanes. On the other hand, on the right, the result of the projection of the dataset onto each one of those one-dimensional hyperplanes is illustrated. Looking at the three, it appears that the hyperplane represented by the solid line is the one that maximizes variance as desired. Therefore, the solid line is the most reasonable selection for a lower dimension. It also finds a second axis (dotted line), orthogonal to the first that accounts for the largest amount of remaining variance.\n\n\nIf we were dealing with higher dimensions, PCA would find more orthogonal axes to the previous axes (for this case a third one if we‚Äôre dealing with 3D); as many axes as the number of dimensions in the dataset. The unit vector that defines the ith axis is called Principal Component (PC). In this case, the first PC is C1 and the second PC is C2.\n\n### Projecting Down to d Dimensions\n\nOnce we‚Äôve identified our principal components it is time to reduce the dimensionality of the dataset down to d dimensions by projecting it to the hyperplane defined by the first d principal components.\n\n\nBasically, we compute the dot product of the training dataset matrix X by the matrix Wd which contains the first d principal components. For this specific case Wd would be composed of the vectors C1 and C2.\n\n\nUsually a good number of dimensions is the one that adds up to a sufficiently large portion of the variance (~95%).\n\n### How is PCA related to SVD?","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"e83656","input":"### Applications of SVD & PCA\n\n\n### 1.EigenFaces\n\nIn the last chapter, we explored image compression. If SVD is applied to a bunch of face shots, we get eigenfaces. Which look creepy, but are really useful for facial detection.\n\n![](http://archive.cnx.org/resources/28b7669c052b1d7ec07962bb69aa5cc3733eb868/PCA_Face.png)\n\n### 2.Google's PageRank\n\nPageRank is a SVD on a markov chain. It helps Google figure out which items to give you to you when you search for something.\n\n### 3.Document Similarity using LSA\n\nLatent Semantic Analysis is just SVD applied to a word/document matrix. It will help us to figure out which types of documents are similar and pick out the document's most important themes. We'll get to this in our next chapter on Natural Language Processing.\n\n### 4.Recommender Systems\nRecommendation systems can be thought of as applying SVD to an User/Item matrix. We'll start on this tomorrow.","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"efaaf0","input":"### Another sparcity example\n\nWe mentioned that as we add more features, the space between the observations grows, and it becomes sparser and sparser. If you need another example of this, think about the unit cube in $n$-dimensions. This is hard to visualize for $n>3$. Algebraically, this is not too difficult to think about. It's just all the $n$-tuples of numbers between 0 and 1.\n\n#### How big is the unit cube?\n\nWhat is the length of the main-diagonal of the $n$-cube? In 2 dimensions, it's $\\sqrt 2$ and in 3 dimensions it's $\\sqrt 3$. In 100-dimensional space, the main diagonal is 10 units long. In 1M-dimensional space, the main diagonal is 1,000 units long. The 1M-dimensional unit cube has points that are pretty far apart from one another, even though all of the individual components of the vectors are bounded between 0 and 1.","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"fbbfd2","input":"### Curse of Dimensionality","pos":0,"type":"cell"}
{"last_load":1575580368137,"type":"file"}