{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "https://towardsdatascience.com/difference-between-batch-gradient-descent-and-stochastic-gradient-descent-1187f1291aa1\n",
    "\n",
    "Recall our standard Gradient Descent (aka batch) algorithm:\n",
    "![image.png](images/sgd1.png)\n",
    "In the above algorithm says, to perform the GD, we need to calculate the gradient of the cost function J. And to calculate the gradient of the cost function, we need to sum (yellow circle!) the cost of each sample. If we have 3 million samples, we have to loop through 3 million times or use the dot product.\n",
    "\n",
    "Another downside of standard gradient descent is you have no guarantee that you will converge to the absolute minimum.\n",
    "\n",
    "An algorithm that will be faster (and has a higher likelihood of converging to the true minimum) is stochastic gradient descent, in which we introduce some randomness:\n",
    "![image.png](images/sgd2.png)\n",
    "Basically, in SGD, we are using the cost gradient of 1 example at each iteration, instead of using the sum of the cost gradient of ALL examples.\n",
    "\n",
    "A few things to note:\n",
    "\n",
    "a) In SGD, before for-looping, you need to randomly shuffle the training examples.\n",
    "\n",
    "b) In SGD, because it’s using only one example at a time, its path to the minima is noisier (more random) than that of the batch gradient. But it’s ok as we are indifferent to the path, as long as it gives us the minimum AND the shorter training time. Here’s a picture to view the difference:\n",
    "![image.png](images/sgd3.png)\n",
    "Or in 3 D:![image.png](images/sgd4.png)\n",
    "\n",
    "c) Mini-batch gradient descent uses n data points (instead of 1 sample in SGD) at each iteration.\n",
    "\n",
    "Some pandas notes to help:\n",
    "1.\tTo shuffle the dataframe and reset the index:\n",
    "\n",
    "```\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "```\n",
    "\n",
    "2.\tTo randomly select one row from an entire dataframe, use:\n",
    "\n",
    "```\n",
    "df.sample() \n",
    "```\n",
    "\n",
    "Or, to randomly select an integer between 0 and n, inclusive, put “import random” in your list of important packages and then type: \n",
    "```\n",
    "random.randint(0,n)\n",
    "```\n",
    "\n",
    "### Exercise 1:\n",
    "Get the SGD algorithm working. To test it, an input of sgd(X,Y,.01,0.00001) gave me out an output of b:86.84622507560407, m:-1.9109285548469945, (although yours will be different due to the randomization).\n",
    "\n",
    "### Exercise 2: \n",
    "Once you have the algorithm working, create a 2D (alpha0, alpha1) plot of both the GD and SGD algorithms. You will see the SGD algorithm take a more meandering path. To speed up the plotting, you will probably only want to plot every 100th point. (You can do this by creating a separate j counting variable and only plot the point if j % 100 == 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Percentage On Time Arrivals']\n",
    "X = df['Mishandled baggage (per 1000 passengers)']\n",
    "\n",
    "def f(x,m,b):                                                         #Defining the function of f\n",
    "     return m*x + b                                                   #Setting the function that is being defined\n",
    "\n",
    "k = len(X)                                                            #Defines the number of values in the X data set\n",
    "def sgd(X, y, alpha, tol):\n",
    "    temp1, temp0 = 1,1                                                #Temporary Variables\n",
    "    theta1, theta0 = 0,0                                              #Initial guesses for m and b \n",
    "\n",
    "    while ((temp0 - theta0)**2 + (temp1 - theta1)**2)**(1/2) > tol:   #Ensures that the loop is defined within the tolerance value \n",
    "        temp1, temp0 = theta1, theta0                                 #Keeps track of what is old and what is updated\n",
    "        deriv0, deriv1 = 0, 0                                         #Resets the for loop\n",
    "        \n",
    "        for i in range(k):                                            #Inner Loop - Runs through k amount of times and spits the output into the while loop\n",
    "            deriv1 += ((f(X[i], theta1, theta0) - y[i])) * X[i]\n",
    "            deriv0 += (f(X[i], theta1, theta0) - y[i])\n",
    "            \n",
    "        theta1 = theta1 - (alpha * deriv1)/k\n",
    "        theta0 = theta0 - (alpha * deriv0)/k\n",
    "    print(theta0, theta1)\n",
    "        \n",
    "gradient(X, y, 0.05,0.0001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
