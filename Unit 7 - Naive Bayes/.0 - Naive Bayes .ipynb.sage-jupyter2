{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":145289216},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1571630641855,"exec_count":1,"id":"9d7994","input":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn import naive_bayes\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split","kernel":"python3","pos":1,"start":1571630635929,"state":"done","type":"cell"}
{"cell_type":"code","end":1571630641894,"exec_count":2,"id":"93849e","input":"df = pd.read_csv(\"data/golf.csv\")\ndf","kernel":"python3","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Outlook</th>\n      <th>Temp</th>\n      <th>Humidity</th>\n      <th>Windy</th>\n      <th>Play Golf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Rainy</td>\n      <td>Hot</td>\n      <td>High</td>\n      <td>False</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>Sunny</td>\n      <td>Hot</td>\n      <td>High</td>\n      <td>True</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>Overcast</td>\n      <td>Hot</td>\n      <td>High</td>\n      <td>False</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>Rainy</td>\n      <td>Mild</td>\n      <td>High</td>\n      <td>False</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>Sunny</td>\n      <td>Cool</td>\n      <td>Normal</td>\n      <td>False</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>Sunny</td>\n      <td>Cool</td>\n      <td>Normal</td>\n      <td>True</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>Overcast</td>\n      <td>Cool</td>\n      <td>Normal</td>\n      <td>True</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>Rainy</td>\n      <td>Mild</td>\n      <td>High</td>\n      <td>False</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>Rainy</td>\n      <td>Cool</td>\n      <td>Normal</td>\n      <td>False</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>Sunny</td>\n      <td>Mild</td>\n      <td>Normal</td>\n      <td>False</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>Rainy</td>\n      <td>Mild</td>\n      <td>Normal</td>\n      <td>True</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>Overcast</td>\n      <td>Mild</td>\n      <td>High</td>\n      <td>True</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>Overcast</td>\n      <td>Hot</td>\n      <td>Normal</td>\n      <td>False</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>Sunny</td>\n      <td>Mild</td>\n      <td>High</td>\n      <td>True</td>\n      <td>No</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"     Outlook  Temp Humidity  Windy Play Golf\n0      Rainy   Hot     High  False        No\n1      Sunny   Hot     High   True        No\n2   Overcast   Hot     High  False       Yes\n3      Rainy  Mild     High  False       Yes\n4      Sunny  Cool   Normal  False       Yes\n5      Sunny  Cool   Normal   True        No\n6   Overcast  Cool   Normal   True       Yes\n7      Rainy  Mild     High  False        No\n8      Rainy  Cool   Normal  False       Yes\n9      Sunny  Mild   Normal  False       Yes\n10     Rainy  Mild   Normal   True       Yes\n11  Overcast  Mild     High   True       Yes\n12  Overcast   Hot   Normal  False       Yes\n13     Sunny  Mild     High   True        No"},"exec_count":2}},"pos":3,"start":1571630641864,"state":"done","type":"cell"}
{"cell_type":"code","end":1571630641941,"exec_count":3,"id":"00999a","input":"df = pd.read_csv('data/votes.csv', index_col = 0)\ndf.head()","kernel":"python3","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>party</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>republican</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>republican</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>democrat</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>democrat</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>democrat</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"        party  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16\n0  republican  0  1  0  1  1  1  0  0  0   1   0   1   1   1   0   1\n1  republican  0  1  0  1  1  1  0  0  0   0   0   1   1   1   0   1\n2    democrat  0  1  1  0  1  1  0  0  0   0   1   0   1   1   0   0\n3    democrat  0  1  1  0  1  1  0  0  0   0   1   0   1   0   0   1\n4    democrat  1  1  1  0  1  1  0  0  0   0   1   0   1   1   1   1"},"exec_count":3}},"pos":7,"start":1571630641920,"state":"done","type":"cell"}
{"cell_type":"code","end":1571630642001,"exec_count":4,"id":"8b2151","input":"X = df.drop(columns = [\"party\"])\ny = df[\"party\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=4444)\n\nmodel = naive_bayes.GaussianNB()\nmodel.fit(X_train,y_train)\n\nprint('train accuracy', model.score(X_train, y_train))\nprint('test accuracy', model.score(X_test, y_test))","kernel":"python3","output":{"0":{"name":"stdout","text":"train accuracy 0.9342105263157895\ntest accuracy 0.9389312977099237\n"}},"pos":9,"start":1571630641949,"state":"done","type":"cell"}
{"cell_type":"code","end":1571630642043,"exec_count":5,"id":"ee5501","input":"confusion_matrix(y_test, model.predict(X_test))","kernel":"python3","output":{"0":{"data":{"text/plain":"array([[75,  2],\n       [ 6, 48]])"},"exec_count":5}},"pos":11,"start":1571630642024,"state":"done","type":"cell"}
{"cell_type":"code","end":1571630642073,"exec_count":6,"id":"795a89","input":"df = df[['party', '1', '2']]\ndf = df.replace(0, \"No\")\ndf = df.replace(1, \"Yes\")\ndf.loc[0, '1'] = 'Maybe'\ndf.head()","kernel":"python3","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>party</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>republican</td>\n      <td>Maybe</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>republican</td>\n      <td>No</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>democrat</td>\n      <td>No</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>democrat</td>\n      <td>No</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>democrat</td>\n      <td>Yes</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"        party      1    2\n0  republican  Maybe  Yes\n1  republican     No  Yes\n2    democrat     No  Yes\n3    democrat     No  Yes\n4    democrat    Yes  Yes"},"exec_count":6}},"pos":13,"start":1571630642049,"state":"done","type":"cell"}
{"cell_type":"code","end":1571630642139,"exec_count":7,"id":"3f9fe6","input":"one_hot = pd.get_dummies(df[['1', '2']])\none_hot.head()","kernel":"python3","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1_Maybe</th>\n      <th>1_No</th>\n      <th>1_Yes</th>\n      <th>2_No</th>\n      <th>2_Yes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   1_Maybe  1_No  1_Yes  2_No  2_Yes\n0        1     0      0     0      1\n1        0     1      0     0      1\n2        0     1      0     0      1\n3        0     1      0     0      1\n4        0     0      1     0      1"},"exec_count":7}},"pos":15,"start":1571630642121,"state":"done","type":"cell"}
{"cell_type":"code","end":1571630642207,"exec_count":8,"id":"162647","input":"X = one_hot\ny = df[\"party\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=.30, random_state=4444)\n\nmodel = naive_bayes.GaussianNB()\nmodel.fit(X_train,y_train)\n\nprint('train accuracy', model.score(X_train, y_train))\nprint('test accuracy', model.score(X_test, y_test))","kernel":"python3","output":{"0":{"name":"stdout","text":"train accuracy 0.6282894736842105\ntest accuracy 0.5877862595419847\n"}},"pos":17,"start":1571630642145,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"27e280","input":"### Using Dummy Variables\n\nThe voting dataset was already nice because the features already took on numeric variables (0's and 1's). What if they didn't? For example, let's change the 0's to No's and the 1's to Yes's. Maybe there were even a few \"maybes\" in there. Let's also reduce our dataset to just two votes for simplicity:","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"38fe7e","input":"In this case, you'll first want to create a one-hot matrix, as we did in the multiple regression chapter:","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"3b8c7c","input":"So basically, P(X|y) here means, the probability of “Not playing golf” given that the weather conditions are “Rainy outlook”, “Temperature is hot”, “high humidity” and “no wind”.\n\nRemember that we are assuming independence, so we can multiply probabilities: $P(X|y) = P(x_1|y)P(x_2|y)...P(x_n|y)$\n\nHence, we reach the result:\n\n$P(y|x_1,...,x_n) = \\frac{ P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)} $\n\nwhich can be expressed as:\n\n$ P(y|x_1,...,x_n) = \\frac{P(y)\\prod_{i=1}^{n}P(x_i|y)}{P(x_1)P(x_2)...P(x_n)} $\n\nNow, as the denominator remains constant for a given input, we can remove that term:\n\n $P(y|x_1,...,x_n)\\propto P(y)\\prod_{i=1}^{n}P(x_i|y) $\n\nNow, we need to create a classifier model. For this, we find the probability of a given set of inputs for all possible values of the class variable y and pick up the output with maximum probability.\n\nLet us try to apply the above formula manually on our weather dataset. For this, we need to do some precomputations on our dataset.\n\nWe need to find $P(x_i | y_j)$ for each $x_i$ in X and $y_j$ in y. All these calculations have been demonstrated in the tables below:\n\n<img src=\"images/golf1.png\" width=\"500\">\n\n<img src=\"images/golf2.png\" width=\"200\">\n\n","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"4dce48","input":"We can also view the confusion matrix to see that 2 republicans were classified incorrectly as democrats and 6 democrats were incorrectly classified as republicans:","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"68a6fb","input":"Sources: \n\nhttps://towardsdatascience.com/what-is-bayes-rule-bb6598d8a2fd\n\nhttps://www.geeksforgeeks.org/naive-bayes-classifiers/\n\n### Independence\nTwo events are independent if the occurrence of one has no effect at all on the probability of the other. If two events are independent, then $P(A \\text{ and } B) = P(A) P(B)$. For example, since the toss of a die is independent from the next toss, then the probability of getting a 1 on my first toss and a 5 on my second toss is $(1/6)*(1/6)=1/36$. In contrast, if I draw two cards from a deck without replacement, then these two events are dependent and thus I cannot say that the probability of getting a queen on my first and second draw is $(4/52)*(4/52)$ since it is actually $(4/52)*(3/51)$.\n\n### Bayes Rule\nBayes rule provides us with a way to update our beliefs based on the arrival of new, relevant pieces of evidence. For example, if we were trying to provide the probability that a given person has cancer, we would initially just say it is whatever percent of the population has cancer. However, given additional evidence such as the fact that the person is a smoker, we can update our probability, since the probability of having cancer is higher given that the person is a smoker. This allows us to utilize prior knowledge to improve our probability estimations.\n\n**Bayes Rule** states that:\n\n$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$\n\nNote that P(A|B) is read as \"the probability of A given B\". The rule has a very simple derivation that directly leads from the relationship between joint and conditional probabilities. First, note that $P(A \\text{ and } B) = P(A|B)P(B)$ and $P( A\\text{ and } B) = P(B|A)P(A)$. Setting these equal to each other and rearranging gives us Bayes rule.\n\nIn this formula, A is the event we want the probability of, and B is the new evidence that is related to A in some way.\n\n\nP(A|B) is called the **posterior**; this is what we are trying to estimate. In the above example, this would be the “probability of having cancer given that the person is a smoker”.\n\n\nP(B|A) is called the **likelihood**; this is the probability of observing the new evidence, given our initial hypothesis. In the above example, this would be the “probability of being a smoker given that the person has cancer”.\n\n\nP(A) is called the **prior**; this is the probability of our hypothesis without any additional prior information. In the above example, this would be the “probability of having cancer”.\n\n\nP(B) is called the **marginal likelihood**; this is the total probability of observing the evidence. In the above example, this would be the “probability of being a smoker”. In many applications of Bayes Rule, this is ignored, as it mainly serves as normalization.\n\n### Bayes Example\nUsing the cancer diagnosis example, we can show that Bayes rule allows us to obtain a much better estimate. Now, we will put some made-up numbers into the example so we can assess the difference that Bayes rule made. Assume that the probability of having cancer is 0.05 — meaning that 5% of people have cancer. Now, assume that the probability of being a smoker is 0.10 — meaning that 10% of people are smokers, and that 20% of people with cancer are smokers, so P(smoker|cancer) = 0.20. Initially, our probability for cancer is simply our prior, so 0.05. However, using new evidence, we can instead calculate P(cancer|smoker), which is equal to:\n\n$(P(smoker|cancer) * P(cancer)) / P(smoker) = (0.20 * 0.05) / (0.10) = 0.10.$\n\n\nBy introducing new evidence, we therefore obtained a better probability estimation. Initially we had a probability of 0.05, but using the smoker evidence, we were able to get to a more accurate probability that was double our prior.\n\n### Naive Bayes\n\nNaive Bayes classifiers are a collection of classification algorithms based on Bayes’ Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other. Since we assume each feature is independent from each other, this is why it is called \"naive\".\n\nConsider a fictional dataset that describes the weather conditions for playing a game of golf. Given the weather conditions, each tuple classifies the conditions as fit (“Yes”) or unfit (“No”) for playing golf. The dataset is below:","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"6ab5cb","input":"Now, you would be ready to run your Naive Bayes classifier:","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"81810f","input":"The dataset is divided into two parts, namely, **feature matrix**, X, and the **response vector**, y.\n\n**Feature matrix** contains all the vectors (rows) of dataset in which each vector consists of the value of dependent features. In above dataset, features are ‘Outlook’, ‘Temperature’, ‘Humidity’ and ‘Windy’.\n\n**Response vector** contains the value of class variable (prediction or output) for each row of feature matrix. In above dataset, the class variable name is ‘Play golf’.\n\nThe fundamental Naive Bayes assumption is that each feature makes an:\n\n- independent\n\n- equal\n\ncontribution to the outcome.\n\nWith relation to our dataset, this concept can be understood as:\n\n- We assume that no pair of features are dependent. For example, the temperature being ‘Hot’ has nothing to do with the humidity or the outlook being ‘Rainy’ has no effect on the winds. Hence, the features are assumed to be independent.\n\n- Secondly, each feature is given the same weight (or importance). For example, knowing only temperature and humidity alone can’t predict the outcome accurately. None of the attributes is irrelevant and the attributes are assumed to be contributing equally to the outcome.\n\n- Note: The assumptions made by Naive Bayes are not generally correct in real world situations. In fact, the independence assumption is never correct but often works well in practice.\n\nNow, with regards to our dataset, we can apply Bayes’ theorem in following way:\n\n $P(y|X) = \\frac{P(X|y) P(y)}{P(X)} $\n\nwhere, y is class variable and X is a dependent feature vector (of size n) where:\n\n $X = (x_1,x_2,x_3,.....,x_n) $\n \n Just to clear, an example of a feature vector and corresponding class variable can be (referring to 1st row of dataset):\n \n```\nX = (Rainy, Hot, High, False)\ny = No\n```\n","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"a0b9ae","input":"Now let's break our dataset up into a test and train set and run the Naive Bayes classifier:","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"af601f","input":"### Naive Bayes\n\nImport the required packages:","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"fcf789","input":"For example, the probability of playing golf given that the temperature is cool, i.e P(play golf = Yes | temp = cool), is 3/9.\n\nAlso, we need to find class probabilities (P(y)) which has been calculated in the table 5. For example, P(play golf = Yes) = 9/14.\n\nSo now, we are done with our pre-computations and the classifier is ready!\n\nLet us test it on a new set of features (let us call it today):\n\n```today = (Rainy, Hot, Normal, True)```\n\nSo, probability of playing golf is given by:\n\n $P(Yes | today) = \\frac{P(Rainy Outlook|Yes)P(Hot Temp|Yes)P(Normal Humidity|Yes)P(Wind|Yes)P(Yes)}{P(today)} $\n\nand probability to not play golf is given by:\n\n $P(No | today) = \\frac{P(Rainy Outlook|No)P(Hot Temp|No)P(Normal Humidity|No)P(Wind|No)P(No)}{P(today)} $\n \n \n Since, P(today) is common in both probabilities, we can ignore P(today) and find proportional probabilities as:\n\n $P(Yes | today) \\propto \\frac{3}{9}.\\frac{2}{9}.\\frac{6}{9}.\\frac{3}{9}.\\frac{9}{14} \\approx 0.0106 $\n\nand\n\n$ P(No | today) \\propto \\frac{2}{5}.\\frac{2}{5}.\\frac{1}{5}.\\frac{3}{5}.\\frac{5}{14} \\approx 0.0069 $\n\nNow, since\n\n $P(Yes | today) + P(No | today) = 1 $\n\nThese numbers can be converted into a probability by making the sum equal to 1 (normalization):\n\n$ P(Yes | today) = \\frac{0.0106}{0.0106 + 0.0069} = 0.61 $\n\nand\n\n$ P(No | today) = \\frac{0.0069}{0.0106 + 0.0069} = 0.39 $\n\nSince\n\n $P(Yes | today) > P(No | today) $\n\nSo, prediction that golf would be played is ‘Yes’.\n\nSome other popular Naive Bayes classifiers are **Multinomial Naive Bayes** and **Bernoulli Naive Bayes**, which are popular for document classification (ex: spam or not spam). We'll get more into these later when we learn about natural language processing.\n\n\nIn spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. They require a small amount of training data to estimate the necessary parameters.\n\n\nNaive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality (which we'll also cover later when we talk more about linear algebra).\n\n### House Votes\nLet's use Naive Bayes to try to predict whether a politician was democrat or republican based on their votes. The real dataset from 1984 is located here:\n\nhttps://archive.ics.uci.edu/ml/datasets/congressional+voting+records\n\nThere are 16 key votes making up the dataset labeled as:\n\n1. handicapped-infants: 2 (y,n) \n2. water-project-cost-sharing: 2 (y,n) \n3. adoption-of-the-budget-resolution: 2 (y,n) \n4. physician-fee-freeze: 2 (y,n) \n5. el-salvador-aid: 2 (y,n) \n6. religious-groups-in-schools: 2 (y,n) \n7. anti-satellite-test-ban: 2 (y,n) \n8. aid-to-nicaraguan-contras: 2 (y,n) \n9. mx-missile: 2 (y,n) \n10. immigration: 2 (y,n) \n11. synfuels-corporation-cutback: 2 (y,n) \n12. education-spending: 2 (y,n) \n13. superfund-right-to-sue: 2 (y,n) \n14. crime: 2 (y,n) \n15. duty-free-exports: 2 (y,n) \n16. export-administration-act-south-africa: 2 (y,n)\n\nHere is the dataset:","pos":6,"type":"cell"}
{"last_load":1571630587796,"type":"file"}