{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/ethandinh/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ethandinh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our example yesterday. We first used the Tfidf Vectorizer to preprocess the data and turn the words into a matrix where uncommon words were granted more weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseball</th>\n",
       "      <th>basketball</th>\n",
       "      <th>broncos</th>\n",
       "      <th>cowboys</th>\n",
       "      <th>cubs</th>\n",
       "      <th>football</th>\n",
       "      <th>giants</th>\n",
       "      <th>hendrix</th>\n",
       "      <th>jagger</th>\n",
       "      <th>jam</th>\n",
       "      <th>joplin</th>\n",
       "      <th>pearl</th>\n",
       "      <th>pop</th>\n",
       "      <th>prince</th>\n",
       "      <th>redsox</th>\n",
       "      <th>rock</th>\n",
       "      <th>stars</th>\n",
       "      <th>tigers</th>\n",
       "      <th>tupac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.479185</td>\n",
       "      <td>0.675356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.397106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.559675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.559675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.464579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.609819</td>\n",
       "      <td>0.609819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.506202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.479185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.675356</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.451635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.544082</td>\n",
       "      <td>0.451635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.544082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473977</td>\n",
       "      <td>0.570997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461804</td>\n",
       "      <td>0.461804</td>\n",
       "      <td>0.461804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   baseball  basketball   broncos   cowboys      cubs  football    giants  \\\n",
       "0  0.479185    0.675356  0.000000  0.000000  0.000000  0.560603  0.000000   \n",
       "1  0.397106    0.000000  0.000000  0.000000  0.559675  0.000000  0.559675   \n",
       "2  0.000000    0.000000  0.609819  0.609819  0.000000  0.506202  0.000000   \n",
       "3  0.479185    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "    hendrix    jagger       jam    joplin     pearl       pop    prince  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.451635  0.000000  0.000000  0.000000  0.000000  0.544082  0.451635   \n",
       "5  0.473977  0.570997  0.000000  0.000000  0.000000  0.000000  0.473977   \n",
       "6  0.000000  0.000000  0.461804  0.461804  0.461804  0.000000  0.000000   \n",
       "\n",
       "     redsox      rock     stars    tigers     tupac  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.464579  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.560603  0.000000  0.000000  0.675356  0.000000  \n",
       "4  0.000000  0.000000  0.544082  0.000000  0.000000  \n",
       "5  0.000000  0.473977  0.000000  0.000000  0.000000  \n",
       "6  0.000000  0.383337  0.000000  0.000000  0.461804  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = ['Football baseball basketball',\n",
    "            'baseball giants cubs redsox',\n",
    "            'football broncos cowboys',\n",
    "            'baseball redsox tigers',\n",
    "            'pop stars hendrix prince',\n",
    "            'hendrix prince jagger rock',\n",
    "            'joplin pearl jam tupac rock',\n",
    "          ]\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=True, \n",
    "                     token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\", \n",
    "                     stop_words=stopwords.words('english'),\n",
    "                     min_df=1)\n",
    "\n",
    "X = vectorizer.fit_transform(articles).toarray()\n",
    "\n",
    "pd.DataFrame(X, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we used an SVD to view the most important words making up each component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseball</th>\n",
       "      <th>basketball</th>\n",
       "      <th>broncos</th>\n",
       "      <th>cowboys</th>\n",
       "      <th>cubs</th>\n",
       "      <th>football</th>\n",
       "      <th>giants</th>\n",
       "      <th>hendrix</th>\n",
       "      <th>jagger</th>\n",
       "      <th>jam</th>\n",
       "      <th>joplin</th>\n",
       "      <th>pearl</th>\n",
       "      <th>pop</th>\n",
       "      <th>prince</th>\n",
       "      <th>redsox</th>\n",
       "      <th>rock</th>\n",
       "      <th>stars</th>\n",
       "      <th>tigers</th>\n",
       "      <th>tupac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>component_1</td>\n",
       "      <td>0.59434</td>\n",
       "      <td>0.26389</td>\n",
       "      <td>0.10775</td>\n",
       "      <td>0.10775</td>\n",
       "      <td>0.25565</td>\n",
       "      <td>0.30849</td>\n",
       "      <td>0.25565</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.47627</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.31811</td>\n",
       "      <td>-0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>component_2</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.51977</td>\n",
       "      <td>0.33357</td>\n",
       "      <td>0.10539</td>\n",
       "      <td>0.10539</td>\n",
       "      <td>0.10539</td>\n",
       "      <td>0.29259</td>\n",
       "      <td>0.51977</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.36438</td>\n",
       "      <td>0.29259</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.10539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             baseball  basketball  broncos  cowboys     cubs  football  \\\n",
       "component_1   0.59434     0.26389  0.10775  0.10775  0.25565   0.30849   \n",
       "component_2   0.00000     0.00000 -0.00000 -0.00000 -0.00000  -0.00000   \n",
       "\n",
       "              giants  hendrix   jagger      jam   joplin    pearl      pop  \\\n",
       "component_1  0.25565 -0.00000 -0.00000 -0.00000 -0.00000 -0.00000 -0.00000   \n",
       "component_2 -0.00000  0.51977  0.33357  0.10539  0.10539  0.10539  0.29259   \n",
       "\n",
       "              prince   redsox     rock    stars   tigers    tupac  \n",
       "component_1 -0.00000  0.47627 -0.00000 -0.00000  0.31811 -0.00000  \n",
       "component_2  0.51977  0.00000  0.36438  0.29259  0.00000  0.10539  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(2)\n",
    "X_svd = svd.fit_transform(X)\n",
    "pd.DataFrame(svd.components_.round(5),\n",
    "             index = [\"component_1\",\"component_2\"],\n",
    "             columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we also wanted to scale our data using Normalizer. This ensures that each vector has a norm of 1. Vectors with a norm of 1 are easy to work with for calculating similarity. We see that each document is a linear combination of the components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component_1</th>\n",
       "      <th>component_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Football baseball basketball</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>baseball giants cubs redsox</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>football broncos cowboys</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>baseball redsox tigers</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pop stars hendrix prince</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hendrix prince jagger rock</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>joplin pearl jam tupac rock</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              component_1  component_2\n",
       "Football baseball basketball          1.0         -0.0\n",
       "baseball giants cubs redsox           1.0         -0.0\n",
       "football broncos cowboys              1.0         -0.0\n",
       "baseball redsox tigers                1.0          0.0\n",
       "pop stars hendrix prince             -0.0          1.0\n",
       "hendrix prince jagger rock           -0.0          1.0\n",
       "joplin pearl jam tupac rock          -0.0          1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_svd = Normalizer(copy=False).fit_transform(X_svd)\n",
    "\n",
    "pd.DataFrame(dtm_svd.round(5),\n",
    "             index=articles, \n",
    "             columns=[\"component_1\",\"component_2\" ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "We can create a pipeline that performs all of the above processes. Let's make the pipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = [('tfidf', TfidfVectorizer(stop_words='english', \n",
    "                        token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\", \n",
    "                        min_df=2)),\n",
    "       ('lsa', TruncatedSVD(2)),\n",
    "       ('normalizer', Normalizer())]\n",
    "pipeline = Pipeline(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put our article data through the pipe to generate the exact table that we made above. The code is a lot cleaner this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component_1</th>\n",
       "      <th>component_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Football baseball basketball</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>baseball giants cubs redsox</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>football broncos cowboys</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>baseball redsox tigers</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pop stars hendrix prince</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hendrix prince jagger rock</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>joplin pearl jam tupac rock</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              component_1  component_2\n",
       "Football baseball basketball          1.0         -0.0\n",
       "baseball giants cubs redsox           1.0          0.0\n",
       "football broncos cowboys              1.0         -0.0\n",
       "baseball redsox tigers                1.0          0.0\n",
       "pop stars hendrix prince              0.0          1.0\n",
       "hendrix prince jagger rock            0.0          1.0\n",
       "joplin pearl jam tupac rock           0.0          1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_svd = pipeline.fit_transform(articles)\n",
    "pd.DataFrame(dtm_svd.round(5),\n",
    "             index=articles, \n",
    "             columns=[\"component_1\",\"component_2\" ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Topic Modeling\n",
    "\n",
    "When we chose two components to use in our SVD, we were essentially choosing to use two topics. We can view the five most important words associated with each topic below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Topic 1:                                    \n",
      "baseball redsox tigers football basketball\n",
      "--------------------------------------------------------------------------------\n",
      "                                    Topic 2:                                    \n",
      "prince hendrix rock jagger stars\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "n_topics = 2\n",
    "n_words = 5\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()                 # get all of the words\n",
    "\n",
    "for topic_num in range(n_topics):        \n",
    "    topic_mat = svd.components_[topic_num]                     # get each row of the SVD truncated matrix\n",
    "\n",
    "    print(f'Topic {topic_num + 1}:'.center(80))\n",
    "\n",
    "    topic_values = sorted(zip(topic_mat, feature_names),       # Sort all of the items in that row \n",
    "                          reverse=True)[:n_words]              # in decending order and keep track of what word\n",
    "                                                               # that value is associated with. Then return the top\n",
    "                                                               # n_words.\n",
    "    \n",
    "    print(' '.join([y for x,y in topic_values]))               # print the output\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Predictive Modeling\n",
    "\n",
    "Here we will train a model on our outcome (Sports 1 or Music 0) and then use the model to predict labels of new sentences. First, let's add a logistic regression classifier to the end of our pipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver=\"lbfgs\")\n",
    "\n",
    "pipeline = Pipeline(pipe + [('model', model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use our previous articles as our training data. (Remember that the first four articles were sports-related and the second three were music-related:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_patt...\n",
       "                 TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5,\n",
       "                              random_state=None, tol=0.0)),\n",
       "                ('normalizer', Normalizer(copy=True, norm='l2')),\n",
       "                ('model',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(articles, [1,1,1,1,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use our model to predict a new sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "new_sentence = 'babe ruth played baseball'\n",
    "y_pred_1 = pipeline.predict([new_sentence])\n",
    "print(y_pred_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, it makes sense that this new article was labeled as sports (1) instead of music (0). What about a new sentence about music?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "new_sentence = 'rock and roll music'\n",
    "y_pred_1 = pipeline.predict([new_sentence])\n",
    "print(y_pred_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would get labeled as music. If we wanted to see the probabilities of this music-related document getting classified as music or sports, we could type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.68091588 0.31908412]]\n"
     ]
    }
   ],
   "source": [
    "new_sentence = 'rock and roll music'\n",
    "y_pred_1 = pipeline.predict_proba([new_sentence])\n",
    "print(y_pred_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Article Suggestions\n",
    "\n",
    "We can use a similar function to the one we used in our recommendation system unit to find articles most similar to a given article. Let's find the sentence most similar to Sentence 0 by calculating the dot product of the rows of the SVD matrix with the row corresponding to Sentence 0 and then sorting in descending order of the dot product value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article #3: baseball redsox tigers. Similarity score: 1.0.\n",
      "Article #2: football broncos cowboys. Similarity score: 1.0.\n",
      "Article #1: baseball giants cubs redsox. Similarity score: 1.0.\n",
      "Article #6: joplin pearl jam tupac rock. Similarity score: -3.578431019537652e-16.\n",
      "Article #5: hendrix prince jagger rock. Similarity score: -5.018099669858968e-16.\n"
     ]
    }
   ],
   "source": [
    "def get_article_recommendations(articles, compare_article, df, num_recom):\n",
    "    recs = []\n",
    "    for article in range(len(df)):\n",
    "        if article != compare_article:\n",
    "            recs.append((np.dot(df[compare_article],df[article]), article))\n",
    "            recs.sort(reverse = True)\n",
    "    for i in range(num_recom):\n",
    "        print(f\"Article #{recs[i][1]}: {articles[recs[i][1]]}. Similarity score: {recs[i][0]}.\")\n",
    "\n",
    "article_num = 0\n",
    "get_article_recommendations(articles, article_num, dtm_svd, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense that Sentences 3, 2, and 1 are most similar to Sentence 0, as they are all sports-related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Sentiment Analysis\n",
    "\n",
    "Natural language processing can handle sentiment analysis. Polarity is near +1 for highly positive sentiment and near -1 for highly negative sentiment. You can look to the compound polarity as a summary of the sentiment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a highly positive sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.302, 'pos': 0.698, 'compound': 0.9107}\n"
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "print(sid.polarity_scores(\"Oh my god I love football, it's so awesome.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a highly negative sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.598, 'neu': 0.402, 'pos': 0.0, 'compound': -0.8147}\n"
     ]
    }
   ],
   "source": [
    "print(sid.polarity_scores(\"I hate swimming it makes me so tired.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "Read more about VADER sentiment analysis here:\n",
    "\n",
    "https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f\n",
    "\n",
    "Then read the actual VADER docs here:\n",
    "\n",
    "http://www.nltk.org/howto/sentiment.html\n",
    "\n",
    "In particular, read all of the \"tricky_sentences\" and then view their sentiment scores further down the page. Sentiment analysis is difficult!\n",
    "\n",
    "The VADER package is not the only package that deals with sentiment analysis. Read about other tools and their pros and cons here:\n",
    "\n",
    "https://medium.com/@b.terryjack/nlp-pre-trained-sentiment-analysis-1eb52a9d742c\n",
    "\n",
    "Comment on what you learned/found interesting on Google Classroom."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "During the elections, millions of Twitter data points, belonging to both Clinton and Trump, were analyzed and classified with a sentiment of either positive, neutral, or negative.\n",
    "\n",
    "The Positive to Negative Tweet ratio was better for Trump than for Clinton.\n",
    "\n",
    "Computers arenâ€™t too comfortable in comprehending Figurative Speech. Figurative language uses words in a way that deviates from their conventionally accepted definitions in order to convey a more complicated meaning or heightened effect. Use of similes, metaphors, hyperboles etc qualify for a figurative speech.\n",
    "\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
