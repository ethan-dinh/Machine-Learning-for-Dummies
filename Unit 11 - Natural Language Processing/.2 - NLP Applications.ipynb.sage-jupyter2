{"backend_state":"running","kernel":"python3","kernel_state":"idle","metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"type":"settings"}
{"cell_type":"code","exec_count":12,"id":"0da056","input":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.pipeline import Pipeline\n\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n#nltk.download('vader_lexicon')","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"6f1afc","input":"articles = ['Football baseball basketball',\n            'baseball giants cubs redsox',\n            'football broncos cowboys',\n            'baseball redsox tigers',\n            'pop stars hendrix prince',\n            'hendrix prince jagger rock',\n            'joplin pearl jam tupac rock',\n          ]\n\nvectorizer = TfidfVectorizer(lowercase=True, \n                     token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\", \n                     stop_words=stopwords.words('english'),\n                     min_df=1)\n\nX = vectorizer.fit_transform(articles).toarray()\n\npd.DataFrame(X,\n             columns=vectorizer.get_feature_names())","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>baseball</th>\n      <th>basketball</th>\n      <th>broncos</th>\n      <th>cowboys</th>\n      <th>cubs</th>\n      <th>football</th>\n      <th>giants</th>\n      <th>hendrix</th>\n      <th>jagger</th>\n      <th>jam</th>\n      <th>joplin</th>\n      <th>pearl</th>\n      <th>pop</th>\n      <th>prince</th>\n      <th>redsox</th>\n      <th>rock</th>\n      <th>stars</th>\n      <th>tigers</th>\n      <th>tupac</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.479185</td>\n      <td>0.675356</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.560603</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.397106</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.559675</td>\n      <td>0.000000</td>\n      <td>0.559675</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.464579</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.609819</td>\n      <td>0.609819</td>\n      <td>0.000000</td>\n      <td>0.506202</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.479185</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.560603</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.675356</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.451635</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.544082</td>\n      <td>0.451635</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.544082</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.473977</td>\n      <td>0.570997</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.473977</td>\n      <td>0.000000</td>\n      <td>0.473977</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.461804</td>\n      <td>0.461804</td>\n      <td>0.461804</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.383337</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.461804</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   baseball  basketball   broncos   cowboys      cubs  football    giants  \\\n0  0.479185    0.675356  0.000000  0.000000  0.000000  0.560603  0.000000   \n1  0.397106    0.000000  0.000000  0.000000  0.559675  0.000000  0.559675   \n2  0.000000    0.000000  0.609819  0.609819  0.000000  0.506202  0.000000   \n3  0.479185    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n4  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n5  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n6  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n\n    hendrix    jagger       jam    joplin     pearl       pop    prince  \\\n0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n4  0.451635  0.000000  0.000000  0.000000  0.000000  0.544082  0.451635   \n5  0.473977  0.570997  0.000000  0.000000  0.000000  0.000000  0.473977   \n6  0.000000  0.000000  0.461804  0.461804  0.461804  0.000000  0.000000   \n\n     redsox      rock     stars    tigers     tupac  \n0  0.000000  0.000000  0.000000  0.000000  0.000000  \n1  0.464579  0.000000  0.000000  0.000000  0.000000  \n2  0.000000  0.000000  0.000000  0.000000  0.000000  \n3  0.560603  0.000000  0.000000  0.675356  0.000000  \n4  0.000000  0.000000  0.544082  0.000000  0.000000  \n5  0.000000  0.473977  0.000000  0.000000  0.000000  \n6  0.000000  0.383337  0.000000  0.000000  0.461804  "},"exec_count":13,"output_type":"execute_result"}},"pos":3,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"949e8e","input":"svd = TruncatedSVD(2)\nX_svd = svd.fit_transform(X)\npd.DataFrame(svd.components_.round(5),\n             index = [\"component_1\",\"component_2\"],\n             columns = vectorizer.get_feature_names())","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>baseball</th>\n      <th>basketball</th>\n      <th>broncos</th>\n      <th>cowboys</th>\n      <th>cubs</th>\n      <th>football</th>\n      <th>giants</th>\n      <th>hendrix</th>\n      <th>jagger</th>\n      <th>jam</th>\n      <th>joplin</th>\n      <th>pearl</th>\n      <th>pop</th>\n      <th>prince</th>\n      <th>redsox</th>\n      <th>rock</th>\n      <th>stars</th>\n      <th>tigers</th>\n      <th>tupac</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>component_1</th>\n      <td>0.59434</td>\n      <td>0.26389</td>\n      <td>0.10775</td>\n      <td>0.10775</td>\n      <td>0.25565</td>\n      <td>0.30849</td>\n      <td>0.25565</td>\n      <td>-0.00000</td>\n      <td>-0.00000</td>\n      <td>-0.00000</td>\n      <td>-0.00000</td>\n      <td>-0.00000</td>\n      <td>-0.00000</td>\n      <td>-0.00000</td>\n      <td>0.47627</td>\n      <td>-0.00000</td>\n      <td>-0.00000</td>\n      <td>0.31811</td>\n      <td>-0.00000</td>\n    </tr>\n    <tr>\n      <th>component_2</th>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>-0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.51977</td>\n      <td>0.33357</td>\n      <td>0.10539</td>\n      <td>0.10539</td>\n      <td>0.10539</td>\n      <td>0.29259</td>\n      <td>0.51977</td>\n      <td>0.00000</td>\n      <td>0.36438</td>\n      <td>0.29259</td>\n      <td>0.00000</td>\n      <td>0.10539</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"             baseball  basketball  broncos  cowboys     cubs  football  \\\ncomponent_1   0.59434     0.26389  0.10775  0.10775  0.25565   0.30849   \ncomponent_2   0.00000     0.00000  0.00000 -0.00000  0.00000   0.00000   \n\n              giants  hendrix   jagger      jam   joplin    pearl      pop  \\\ncomponent_1  0.25565 -0.00000 -0.00000 -0.00000 -0.00000 -0.00000 -0.00000   \ncomponent_2  0.00000  0.51977  0.33357  0.10539  0.10539  0.10539  0.29259   \n\n              prince   redsox     rock    stars   tigers    tupac  \ncomponent_1 -0.00000  0.47627 -0.00000 -0.00000  0.31811 -0.00000  \ncomponent_2  0.51977  0.00000  0.36438  0.29259  0.00000  0.10539  "},"exec_count":14,"output_type":"execute_result"}},"pos":5,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"c6d7fd","input":"dtm_svd = Normalizer(copy=False).fit_transform(X_svd)\n\npd.DataFrame(dtm_svd.round(5),\n             index=articles, \n             columns=[\"component_1\",\"component_2\" ])","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>component_1</th>\n      <th>component_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Football baseball basketball</th>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>baseball giants cubs redsox</th>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>football broncos cowboys</th>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>baseball redsox tigers</th>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>pop stars hendrix prince</th>\n      <td>-0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>hendrix prince jagger rock</th>\n      <td>-0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>joplin pearl jam tupac rock</th>\n      <td>-0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"                              component_1  component_2\nFootball baseball basketball          1.0          0.0\nbaseball giants cubs redsox           1.0          0.0\nfootball broncos cowboys              1.0          0.0\nbaseball redsox tigers                1.0          0.0\npop stars hendrix prince             -0.0          1.0\nhendrix prince jagger rock           -0.0          1.0\njoplin pearl jam tupac rock          -0.0          1.0"},"exec_count":15,"output_type":"execute_result"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":16,"id":"153fa5","input":"pipe = [('tfidf', TfidfVectorizer(stop_words='english', \n                        token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\", \n                        min_df=2)),\n       ('lsa', TruncatedSVD(2)),\n       ('normalizer', Normalizer())]\npipeline = Pipeline(pipe)","pos":9,"type":"cell"}
{"cell_type":"code","exec_count":17,"id":"fb504b","input":"dtm_svd = pipeline.fit_transform(articles)\npd.DataFrame(dtm_svd.round(5),\n             index=articles, \n             columns=[\"component_1\",\"component_2\" ])","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>component_1</th>\n      <th>component_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Football baseball basketball</th>\n      <td>1.0</td>\n      <td>-0.0</td>\n    </tr>\n    <tr>\n      <th>baseball giants cubs redsox</th>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>football broncos cowboys</th>\n      <td>1.0</td>\n      <td>-0.0</td>\n    </tr>\n    <tr>\n      <th>baseball redsox tigers</th>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>pop stars hendrix prince</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>hendrix prince jagger rock</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>joplin pearl jam tupac rock</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"                              component_1  component_2\nFootball baseball basketball          1.0         -0.0\nbaseball giants cubs redsox           1.0          0.0\nfootball broncos cowboys              1.0         -0.0\nbaseball redsox tigers                1.0          0.0\npop stars hendrix prince              0.0          1.0\nhendrix prince jagger rock            0.0          1.0\njoplin pearl jam tupac rock           0.0          1.0"},"exec_count":17,"output_type":"execute_result"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":18,"id":"910cd1","input":"n_topics = 2\nn_words = 5\n\nfeature_names = vectorizer.get_feature_names()                 # get all of the words\n\nfor topic_num in range(n_topics):        \n    topic_mat = svd.components_[topic_num]                     # get each row of the SVD truncated matrix\n\n    print(f'Topic {topic_num + 1}:'.center(80))\n\n    topic_values = sorted(zip(topic_mat, feature_names),       # Sort all of the items in that row \n                          reverse=True)[:n_words]              # in decending order and keep track of what word\n                                                               # that value is associated with. Then return the top\n                                                               # n_words.\n    \n    print(' '.join([y for x,y in topic_values]))               # print the output\n    print('-'*80)","output":{"0":{"name":"stdout","output_type":"stream","text":"                                    Topic 1:                                    \nbaseball redsox tigers football basketball\n--------------------------------------------------------------------------------\n                                    Topic 2:                                    \nprince hendrix rock jagger stars\n--------------------------------------------------------------------------------\n"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":24,"id":"29b594","input":"sid = SentimentIntensityAnalyzer()\nprint(sid.polarity_scores(\"Oh my god I love football, it's so awesome.\"))","output":{"0":{"name":"stdout","output_type":"stream","text":"{'neg': 0.0, 'neu': 0.302, 'pos': 0.698, 'compound': 0.9107}\n"}},"pos":29,"type":"cell"}
{"cell_type":"code","exec_count":25,"id":"c0de4f","input":"print(sid.polarity_scores(\"I hate swimming it makes me so tired.\"))","output":{"0":{"name":"stdout","output_type":"stream","text":"{'neg': 0.598, 'neu': 0.402, 'pos': 0.0, 'compound': -0.8147}\n"}},"pos":31,"type":"cell"}
{"cell_type":"code","exec_count":29,"id":"4b0cda","input":"model = LogisticRegression(solver=\"lbfgs\")\n\npipeline = Pipeline(pipe + [('model', model)])","pos":15,"type":"cell"}
{"cell_type":"code","exec_count":30,"id":"63c1e8","input":"pipeline.fit(articles, [1,1,1,1,0,0,0])","output":{"0":{"data":{"text/plain":"Pipeline(memory=None,\n     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...enalty='l2', random_state=None, solver='lbfgs',\n          tol=0.0001, verbose=0, warm_start=False))])"},"exec_count":30,"output_type":"execute_result"}},"pos":17,"type":"cell"}
{"cell_type":"code","exec_count":31,"id":"c5fe04","input":"new_sentence = 'babe ruth played baseball'\ny_pred_1 = pipeline.predict([new_sentence])\nprint(y_pred_1)","output":{"0":{"name":"stdout","output_type":"stream","text":"[1]\n"}},"pos":19,"type":"cell"}
{"cell_type":"code","exec_count":32,"id":"59c790","input":"new_sentence = 'rock and roll music'\ny_pred_1 = pipeline.predict([new_sentence])\nprint(y_pred_1)","output":{"0":{"name":"stdout","output_type":"stream","text":"[0]\n"}},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":33,"id":"b6191f","input":"new_sentence = 'rock and roll music'\ny_pred_1 = pipeline.predict_proba([new_sentence])\nprint(y_pred_1)","output":{"0":{"name":"stdout","output_type":"stream","text":"[[0.68091588 0.31908412]]\n"}},"pos":23,"type":"cell"}
{"cell_type":"code","exec_count":34,"id":"92003a","input":"def get_article_recommendations(articles, compare_article, df, num_recom):\n    recs = []\n    for article in range(len(df)):\n        if article != compare_article:\n            recs.append((np.dot(df[compare_article],df[article]), article))\n            recs.sort(reverse = True)\n    for i in range(num_recom):\n        print(f\"Article #{recs[i][1]}: {articles[recs[i][1]]}. Similarity score: {recs[i][0]}.\")\n\narticle_num = 0\nget_article_recommendations(articles, article_num, dtm_svd, 5)","output":{"0":{"name":"stdout","output_type":"stream","text":"Article #3: baseball redsox tigers. Similarity score: 1.0.\nArticle #2: football broncos cowboys. Similarity score: 1.0.\nArticle #1: baseball giants cubs redsox. Similarity score: 1.0.\nArticle #4: pop stars hendrix prince. Similarity score: -2.2197667493164922e-15.\nArticle #5: hendrix prince jagger rock. Similarity score: -2.3524011701741997e-15.\n"}},"pos":25,"type":"cell"}
{"cell_type":"code","id":"d1338a","input":"","pos":33,"type":"cell"}
{"cell_type":"markdown","id":"0316a3","input":"Now we can put our article data through the pipe to generate the exact table that we made above. The code is a lot cleaner this way:","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"25c738","input":"This would get labeled as music. If we wanted to see the probabilities of this music-related document getting classified as music or sports, we could type:","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"4d1256","input":"It makes sense that Sentences 3, 2, and 1 are most similar to Sentence 0, as they are all sports-related.","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"52d936","input":"### 3.Article Suggestions\n\nWe can use a similar function to the one we used in our recommendation system unit to find articles most similar to a given article. Let's find the sentence most similar to Sentence 0 by calculating the dot product of the rows of the SVD matrix with the row corresponding to Sentence 0 and then sorting in descending order of the dot product value:","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"56ee6d","input":"### 4.Sentiment Analysis\n\nNatural language processing can handle sentiment analysis. Polarity is near +1 for highly positive sentiment and near -1 for highly negative sentiment. You can look to the compound polarity as a summary of the sentiment:","pos":27,"type":"cell"}
{"cell_type":"markdown","id":"771147","input":"Then we also wanted to scale our data using Normalizer. This ensures that each vector has a norm of 1. Vectors with a norm of 1 are easy to work with for calculating similarity. We see that each document is a linear combination of the components:","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"7f58d6","input":"Run the cell below to import the required packages:","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"832487","input":"Now, we can use our previous articles as our training data. (Remember that the first four articles were sports-related and the second three were music-related:","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"8558bc","input":"### 1.Topic Modeling\n\nWhen we chose two components to use in our SVD, we were essentially choosing to use two topics. We can view the five most important words associated with each topic below:","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"8697f2","input":"### 2.Predictive Modeling\n\nHere we will train a model on our outcome (Sports 1 or Music 0) and then use the model to predict labels of new sentences. First, let's add a logistic regression classifier to the end of our pipe:","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"9e7f2b","input":"Here's a highly negative sentence:","pos":30,"type":"cell"}
{"cell_type":"markdown","id":"a465ce","input":"### Pipelines\n\nWe can create a pipeline that performs all of the above processes. Let's make the pipe:","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"aa28cc","input":"Here's a highly positive sentence:","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"b2f9ea","input":"Then, we used an SVD to view the most important words making up each component:","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"bcc7a7","input":"Now, let's use our model to predict a new sentence:","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"cf9e2c","input":"Clearly, it makes sense that this new article was labeled as sports (1) instead of music (0). What about a new sentence about music?","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"d8ea6e","input":"Recall our example yesterday. We first used the Tfidf Vectorizer to preprocess the data and turn the words into a matrix where uncommon words were granted more weight:","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"efb326","input":"### Homework\n\nRead more about VADER sentiment analysis here:\n\nhttps://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f\n\nThen read the actual VADER docs here:\n\nhttp://www.nltk.org/howto/sentiment.html\n\nIn particular, read all of the \"tricky_sentences\" and then view their sentiment scores further down the page. Sentiment analysis is difficult!\n\nThe VADER package is not the only package that deals with sentiment analysis. Read about other tools and their pros and cons here:\n\nhttps://medium.com/@b.terryjack/nlp-pre-trained-sentiment-analysis-1eb52a9d742c\n\nComment on what you learned/found interesting on Google Classroom.","pos":32,"type":"cell"}
{"last_load":1576602351687,"type":"file"}